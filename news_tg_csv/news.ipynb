{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Настройки отображения датафреймов\n",
    "pd.set_option('display.max_rows', None)  # Показывать все строки\n",
    "pd.set_option('display.max_columns', None)  # Показывать все столбцы\n",
    "pd.set_option('display.width', None)  # Автоматическая ширина\n",
    "pd.set_option('display.max_colwidth', None)  # Показывать полное содержимое ячеек\n",
    "pd.set_option('display.expand_frame_repr', True)  # Разрешить перенос строк"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tg_chat(chat_path):\n",
    "    # Reload the file\n",
    "    with open(chat_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Extract relevant data\n",
    "    messages = data.get(\"messages\", [])\n",
    "    cleaned_data = []\n",
    "\n",
    "    # Паттерн для поиска URL\n",
    "    url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "\n",
    "    for msg in messages:\n",
    "        if msg.get(\"type\") == \"message\":\n",
    "            text_content = \"\"\n",
    "            if isinstance(msg.get(\"text\"), list):\n",
    "                text_content = \"\".join([seg[\"text\"] if isinstance(seg, dict) else seg for seg in msg[\"text\"]])\n",
    "            elif isinstance(msg.get(\"text\"), str):\n",
    "                text_content = msg[\"text\"]\n",
    "                \n",
    "            # Заменяем символы \\n на пустую строку\n",
    "            text_content = text_content.replace('\\n', '')\n",
    "            \n",
    "            # Удаляем URL из текста\n",
    "            text_content = re.sub(url_pattern, '', text_content)\n",
    "            \n",
    "            # Добавляем строку только если текст не пустой\n",
    "            if text_content.strip():\n",
    "                cleaned_data.append({\n",
    "                    \"date\": msg.get(\"date\").split('T')[0],\n",
    "                    \"time\": msg.get(\"date\", \"\").split(\"T\")[1] if \"T\" in msg.get(\"date\", \"\") else \"\",\n",
    "                    \"text\": text_content.strip()  # Удаляем лишние пробелы в начале и конце\n",
    "                })\n",
    "\n",
    "    # Create a dataframe and remove any remaining empty rows\n",
    "    df = pd.DataFrame(cleaned_data)\n",
    "    df = df.dropna(subset=['text'])\n",
    "    df = df[df['text'].str.strip() != '']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_json_directory(input_dir, output_dir):\n",
    "    '''перевод файлов json в csv из одной директории в другую'''\n",
    "    # Создаем выходную директорию, если она не существует\n",
    "    # Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Получаем список всех JSON файлов в указанной директории\n",
    "    json_files = [f for f in os.listdir(input_dir) if f.endswith('.json')]\n",
    "    \n",
    "    for json_file in json_files:\n",
    "        try:\n",
    "            # Полный путь к входному файлу\n",
    "            input_path = os.path.join(input_dir, json_file)\n",
    "            \n",
    "            # Создаем имя выходного файла, заменяя расширение .json на .csv\n",
    "            output_filename = os.path.splitext(json_file)[0] + '.csv'\n",
    "            output_path = os.path.join(output_dir, output_filename)\n",
    "            \n",
    "            # Обрабатываем файл и сохраняем результат\n",
    "            df = get_tg_chat(input_path)\n",
    "            df.to_csv(output_path, index=False, encoding='utf-8')\n",
    "            \n",
    "            print(f\"Успешно обработан файл: {json_file}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка при обработке файла {json_file}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_news_csv(directory_path, fail_name):\n",
    "    '''Объеденение новостей в выбранной директории и названием'''\n",
    "    # Получаем список всех CSV файлов в указанной директории\n",
    "    csv_files = glob.glob(os.path.join(directory_path, '*.csv'))\n",
    "    \n",
    "    # Создаем пустой список для хранения датафреймов\n",
    "    dfs = []\n",
    "    \n",
    "    # Читаем каждый CSV файл и добавляем его в список\n",
    "    for csv_file in csv_files:\n",
    "        df = pd.read_csv(csv_file)\n",
    "        dfs.append(df)\n",
    "    \n",
    "    # Объединяем все датафреймы в один\n",
    "    merged_df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    # Сортируем по дате и времени\n",
    "    merged_df = merged_df.sort_values(by=['date', 'time'])\n",
    "    \n",
    "    # Сохраняем результат в новый CSV файл\n",
    "    merged_df.to_csv(f'merged_df/{fail_name}.csv', index=False)\n",
    "    \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_json_directory(\"news_tg\", \"news_tg_csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_json_directory(\"chanels_tg\", \"chanels_tg_csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merge_news_csv('news_tg_csv', 'news_tg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_blogs = merge_news_csv('chanels_tg_csv', 'chanels_tg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_blogs[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('merged_df/news_tg.csv')\n",
    "chunk_news = df[-500:]\n",
    "chunk_news.to_csv('chunk_news.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chunk_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка данных\n",
    "data_path = \"interfax_history.csv\"  # Укажите путь к вашему файлу\n",
    "news_data = pd.read_csv(data_path)\n",
    "\n",
    "# Фильтрация новостей, связанных с Газпромом\n",
    "gazprom_keywords = ['Газпром', 'газ', 'газовый', 'нефть', 'газпром']\n",
    "filtered_news = news_data[news_data['text'].str.contains('|'.join(gazprom_keywords), case=False, na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict\n",
    "\n",
    "# Список русских стоп-слов\n",
    "RUSSIAN_STOP_WORDS = [\n",
    "    'и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она',\n",
    "    'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее',\n",
    "    'мне', 'было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда',\n",
    "    'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или', 'ни', 'быть', 'был', 'него', 'до',\n",
    "    'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего',\n",
    "    'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя',\n",
    "    'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе',\n",
    "    'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой',\n",
    "    'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее',\n",
    "    'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при', 'наконец',\n",
    "    'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти',\n",
    "    'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве', 'три', 'эту', 'моя',\n",
    "    'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть',\n",
    "    'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между'\n",
    "]\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Предварительная обработка текста\"\"\"\n",
    "    # Приводим к нижнему регистру\n",
    "    text = text.lower()\n",
    "    # Удаляем все символы кроме букв и пробелов\n",
    "    text = ''.join(char for char in text if char.isalnum() or char.isspace())\n",
    "    return text\n",
    "\n",
    "def find_duplicates_in_window(df, window_days=2, similarity_threshold=0.6):\n",
    "    \"\"\"\n",
    "    Находит дубликаты новостей в заданном временном окне используя TF-IDF и косинусное сходство\n",
    "    \"\"\"\n",
    "    # Создаем копию датафрейма\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Создаем столбец с datetime и сортируем\n",
    "    df['datetime'] = pd.to_datetime(df['date'] + ' ' + df['time'])\n",
    "    df = df.sort_values('datetime')\n",
    "    \n",
    "    # Предобработка текстов\n",
    "    df['processed_text'] = df['text'].apply(preprocess_text)\n",
    "    \n",
    "    # Создаем и обучаем TF-IDF векторайзер на всем датасете\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        min_df=1,\n",
    "        stop_words=RUSSIAN_STOP_WORDS,  # Теперь передаем список\n",
    "        ngram_range=(1, 2)  # Добавляем биграммы для лучшего сравнения\n",
    "    )\n",
    "    tfidf_matrix_full = vectorizer.fit_transform(df['processed_text'])\n",
    "    \n",
    "    # Словарь для хранения групп похожих новостей\n",
    "    duplicate_groups = defaultdict(set)\n",
    "    \n",
    "    # Множество для отслеживания обработанных индексов\n",
    "    processed_indices = set()\n",
    "    \n",
    "    # Проходим по каждой новости\n",
    "    for i in range(len(df)):\n",
    "        current_idx = df.index[i]\n",
    "        \n",
    "        # Пропускаем, если новость уже в какой-то группе дубликатов\n",
    "        if current_idx in processed_indices:\n",
    "            continue\n",
    "            \n",
    "        current_date = df.iloc[i]['datetime']\n",
    "        \n",
    "        # Определяем временное окно\n",
    "        date_min = current_date - timedelta(days=window_days)\n",
    "        date_max = current_date + timedelta(days=window_days)\n",
    "        \n",
    "        # Выбираем необработанные новости в окне\n",
    "        mask_window = (\n",
    "            (df['datetime'] >= date_min) & \n",
    "            (df['datetime'] <= date_max) & \n",
    "            ~df.index.isin(processed_indices)\n",
    "        )\n",
    "        window_indices = df[mask_window].index\n",
    "        \n",
    "        if len(window_indices) > 1:\n",
    "            # Получаем векторы TF-IDF для текущего окна\n",
    "            current_idx_in_window = np.where(window_indices == current_idx)[0][0]\n",
    "            window_tfidf = tfidf_matrix_full[window_indices]\n",
    "            \n",
    "            # Вычисляем сходство только для текущего окна\n",
    "            similarities = cosine_similarity(\n",
    "                window_tfidf[current_idx_in_window:current_idx_in_window+1], \n",
    "                window_tfidf\n",
    "            )[0]\n",
    "            \n",
    "            # Находим похожие новости\n",
    "            similar_indices = np.where(similarities > similarity_threshold)[0]\n",
    "            \n",
    "            if len(similar_indices) > 1:  # Если есть похожие новости\n",
    "                # Получаем реальные индексы из исходного датафрейма\n",
    "                similar_indices_full = window_indices[similar_indices]\n",
    "                \n",
    "                # Находим самую раннюю новость в группе\n",
    "                earliest_news = df.loc[similar_indices_full]\n",
    "                earliest_idx = earliest_news['datetime'].idxmin()\n",
    "                \n",
    "                # Добавляем все похожие новости в группу\n",
    "                duplicate_groups[earliest_idx].update(\n",
    "                    idx for idx in similar_indices_full if idx != earliest_idx\n",
    "                )\n",
    "                \n",
    "                # Отмечаем все найденные индексы как обработанные\n",
    "                processed_indices.update(similar_indices_full)\n",
    "    \n",
    "    # Собираем все дубликаты в одно множество\n",
    "    all_duplicates = set()\n",
    "    for duplicates in duplicate_groups.values():\n",
    "        all_duplicates.update(duplicates)\n",
    "    \n",
    "    # Удаляем найденные дубликаты и ненужные столбцы\n",
    "    df_cleaned = df.drop(index=list(all_duplicates))\n",
    "    df_cleaned = df_cleaned.drop(['datetime', 'processed_text'], axis=1)\n",
    "    \n",
    "    print(f\"Найдено {len(all_duplicates)} дубликатов в {len(duplicate_groups)} группах\")\n",
    "    print(f\"Осталось {len(df_cleaned)} уникальных новостей\")\n",
    "    \n",
    "    # Опционально: выводим пример группы похожих новостей\n",
    "    if len(duplicate_groups) > 0:\n",
    "        print(\"\\nПример группы похожих новостей:\")\n",
    "        first_group_key = next(iter(duplicate_groups))\n",
    "        first_group = [first_group_key] + list(duplicate_groups[first_group_key])\n",
    "        for idx in first_group:\n",
    "            print(f\"[{df.loc[idx, 'datetime']}] {df.loc[idx, 'text'][:100]}...\")\n",
    "    \n",
    "    return df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    # Читаем CSV файл\n",
    "    df = pd.read_csv('merged_df/news_tg.csv')\n",
    "    \n",
    "    # Удаляем дубликаты с временным окном в 2 дня\n",
    "    df_cleaned = find_duplicates_in_window(df, window_days=2)\n",
    "    \n",
    "    # Сохраняем результат\n",
    "    df_cleaned.to_csv('unique_news.csv', index=False)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('unique_news.csv')\n",
    "df[-50:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
