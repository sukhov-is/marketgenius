{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Список русских стоп-слов\n",
    "RUSSIAN_STOP_WORDS = [\n",
    "    'и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она',\n",
    "    'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее',\n",
    "    'мне', 'было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда',\n",
    "    'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или', 'ни', 'быть', 'был', 'него', 'до',\n",
    "    'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего',\n",
    "    'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя',\n",
    "    'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе',\n",
    "    'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой',\n",
    "    'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее',\n",
    "    'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при', 'наконец',\n",
    "    'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти',\n",
    "    'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве', 'три', 'эту', 'моя',\n",
    "    'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть',\n",
    "    'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между'\n",
    "]\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Предварительная обработка текста\"\"\"\n",
    "    # Приводим к нижнему регистру\n",
    "    text = text.lower()\n",
    "    # Удаляем все символы кроме букв и пробелов\n",
    "    text = ''.join(char for char in text if char.isalnum() or char.isspace())\n",
    "    return text\n",
    "\n",
    "def find_duplicates_in_window(df, window_days=2, similarity_threshold=0.6):\n",
    "    \"\"\"\n",
    "    Находит дубликаты новостей в заданном временном окне используя TF-IDF и косинусное сходство\n",
    "    \"\"\"\n",
    "    # Создаем копию датафрейма\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Создаем столбец с datetime и сортируем\n",
    "    df['datetime'] = pd.to_datetime(df['date'] + ' ' + df['time'])\n",
    "    df = df.sort_values('datetime')\n",
    "    \n",
    "    # Предобработка текстов\n",
    "    df['processed_text'] = df['text'].apply(preprocess_text)\n",
    "    \n",
    "    # Создаем и обучаем TF-IDF векторайзер на всем датасете\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        min_df=1,\n",
    "        stop_words=RUSSIAN_STOP_WORDS,  # Теперь передаем список\n",
    "        ngram_range=(1, 2)  # Добавляем биграммы для лучшего сравнения\n",
    "    )\n",
    "    tfidf_matrix_full = vectorizer.fit_transform(df['processed_text'])\n",
    "    \n",
    "    # Словарь для хранения групп похожих новостей\n",
    "    duplicate_groups = defaultdict(set)\n",
    "    \n",
    "    # Множество для отслеживания обработанных индексов\n",
    "    processed_indices = set()\n",
    "    \n",
    "    # Проходим по каждой новости\n",
    "    for i in range(len(df)):\n",
    "        current_idx = df.index[i]\n",
    "        \n",
    "        # Пропускаем, если новость уже в какой-то группе дубликатов\n",
    "        if current_idx in processed_indices:\n",
    "            continue\n",
    "            \n",
    "        current_date = df.iloc[i]['datetime']\n",
    "        \n",
    "        # Определяем временное окно\n",
    "        date_min = current_date - timedelta(days=window_days)\n",
    "        date_max = current_date + timedelta(days=window_days)\n",
    "        \n",
    "        # Выбираем необработанные новости в окне\n",
    "        mask_window = (\n",
    "            (df['datetime'] >= date_min) & \n",
    "            (df['datetime'] <= date_max) & \n",
    "            ~df.index.isin(processed_indices)\n",
    "        )\n",
    "        window_indices = df[mask_window].index\n",
    "        \n",
    "        if len(window_indices) > 1:\n",
    "            # Получаем векторы TF-IDF для текущего окна\n",
    "            current_idx_in_window = np.where(window_indices == current_idx)[0][0]\n",
    "            window_tfidf = tfidf_matrix_full[window_indices]\n",
    "            \n",
    "            # Вычисляем сходство только для текущего окна\n",
    "            similarities = cosine_similarity(\n",
    "                window_tfidf[current_idx_in_window:current_idx_in_window+1], \n",
    "                window_tfidf\n",
    "            )[0]\n",
    "            \n",
    "            # Находим похожие новости\n",
    "            similar_indices = np.where(similarities > similarity_threshold)[0]\n",
    "            \n",
    "            if len(similar_indices) > 1:  # Если есть похожие новости\n",
    "                # Получаем реальные индексы из исходного датафрейма\n",
    "                similar_indices_full = window_indices[similar_indices]\n",
    "                \n",
    "                # Находим самую раннюю новость в группе\n",
    "                earliest_news = df.loc[similar_indices_full]\n",
    "                earliest_idx = earliest_news['datetime'].idxmin()\n",
    "                \n",
    "                # Добавляем все похожие новости в группу\n",
    "                duplicate_groups[earliest_idx].update(\n",
    "                    idx for idx in similar_indices_full if idx != earliest_idx\n",
    "                )\n",
    "                \n",
    "                # Отмечаем все найденные индексы как обработанные\n",
    "                processed_indices.update(similar_indices_full)\n",
    "    \n",
    "    # Собираем все дубликаты в одно множество\n",
    "    all_duplicates = set()\n",
    "    for duplicates in duplicate_groups.values():\n",
    "        all_duplicates.update(duplicates)\n",
    "    \n",
    "    # Удаляем найденные дубликаты и ненужные столбцы\n",
    "    df_cleaned = df.drop(index=list(all_duplicates))\n",
    "    df_cleaned = df_cleaned.drop(['datetime', 'processed_text'], axis=1)\n",
    "    \n",
    "    print(f\"Найдено {len(all_duplicates)} дубликатов в {len(duplicate_groups)} группах\")\n",
    "    print(f\"Осталось {len(df_cleaned)} уникальных новостей\")\n",
    "    \n",
    "    # Опционально: выводим пример группы похожих новостей\n",
    "    if len(duplicate_groups) > 0:\n",
    "        print(\"\\nПример группы похожих новостей:\")\n",
    "        first_group_key = next(iter(duplicate_groups))\n",
    "        first_group = [first_group_key] + list(duplicate_groups[first_group_key])\n",
    "        for idx in first_group:\n",
    "            print(f\"[{df.loc[idx, 'datetime']}] {df.loc[idx, 'text'][:100]}...\")\n",
    "    \n",
    "    return df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Читаем CSV файл\n",
    "    df = pd.read_csv('market_impact_news.csv')\n",
    "    \n",
    "    # Удаляем дубликаты с временным окном в 2 дня\n",
    "    df_cleaned = find_duplicates_in_window(df, window_days=2)\n",
    "    \n",
    "    # Сохраняем результат\n",
    "    df_cleaned.to_csv('unique_news.csv', index=False)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
