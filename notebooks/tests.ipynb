{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Загрузка TSV файла\n",
    "df = pd.read_csv(\"data/external/news_tg_csv/telegram_news.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Сохранение в CSV\n",
    "df.to_csv(\"data/external/news_tg_csv/telegram_news_2.сsv\", sep=\",\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "input_file = \"../data/100000_with_emb.csv\"\n",
    "output_file = \"../data/30000_with_emb.csv\"\n",
    "\n",
    "# Открываем входной файл\n",
    "with open(input_file, \"r\", newline=\"\", encoding=\"utf-8\") as infile:\n",
    "    reader = list(csv.reader(infile))  # Читаем все строки в список\n",
    "\n",
    "# Берем заголовок и последние 10 000 строк\n",
    "header = reader[0]  # Заголовок (первая строка)\n",
    "last_5000_rows = reader[-30000:]  # Последние 10 000 строк\n",
    "\n",
    "# Записываем в новый файл\n",
    "with open(output_file, \"w\", newline=\"\", encoding=\"utf-8\") as outfile:\n",
    "    writer = csv.writer(outfile)\n",
    "    writer.writerow(header)  # Записываем заголовок\n",
    "    writer.writerows(last_5000_rows)  # Записываем данные\n",
    "\n",
    "print(\"Готово! Последние 10 000 строк сохранены в\", output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Пусть input.csv — исходный файл, а output.csv — результирующий файл\n",
    "input_file = 'input.csv'\n",
    "output_file = 'output.csv'\n",
    "\n",
    "# Список названий, при наличии которых строка будет удалена\n",
    "names_to_remove = [\"Раньше всех. Ну почти.\", \"ТАСС\", \"Интерфакс\"]\n",
    "\n",
    "# Читаем CSV файл\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# Фильтруем строки, оставляя только те, у которых в столбце channel_name нет указанных названий\n",
    "filtered_df = df[~df['channel_name'].isin(names_to_remove)]\n",
    "\n",
    "# Сохраняем результат в новый CSV файл (без добавления индекса)\n",
    "filtered_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Файл '{output_file}' успешно создан!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"../data/external/news_tg_csv/telegram_news.сsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['datetime'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"../data/raw/news_with_emb.csv\")\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Устанавливаем даты\n",
    "end_date = datetime.now()\n",
    "start_date = end_date - timedelta(days=6*365)\n",
    "\n",
    "# Формируем URL для API ЦБ РФ\n",
    "url = f'https://cbr.ru/hd_base/KeyRate/?UniDbQuery.Posted=True&UniDbQuery.From={start_date.strftime(\"%d.%m.%Y\")}&UniDbQuery.To={end_date.strftime(\"%d.%m.%Y\")}'\n",
    "\n",
    "# Получаем страницу и парсим таблицу\n",
    "response = requests.get(url)\n",
    "tables = pd.read_html(response.text)\n",
    "\n",
    "# Выбираем нужную таблицу\n",
    "key_rate_table = tables[0]\n",
    "\n",
    "# Сохраняем данные в CSV\n",
    "key_rate_table.to_csv('key_rate_cbr.csv', index=False)\n",
    "\n",
    "print(\"Данные успешно загружены и сохранены в файл 'key_rate_cbr.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def get_urals_prices_cbr():\n",
    "    # Формируем даты для запроса\n",
    "    end_date = datetime.now()\n",
    "    start_date = end_date - timedelta(days=365*10)  # за последние 10 лет\n",
    "    \n",
    "    # Формируем URL запроса\n",
    "    url = \"http://www.cbr.ru/scripts/XML_dynamic.asp\"\n",
    "    params = {\n",
    "        'date_req1': start_date.strftime('%d/%m/%Y'),\n",
    "        'date_req2': end_date.strftime('%d/%m/%Y'),\n",
    "        'VAL_NM_RQ': 'R01239'  # Код для нефти Urals\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Выполняем запрос\n",
    "        response = requests.get(url, params=params)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Парсим XML\n",
    "        root = ET.fromstring(response.content)\n",
    "        \n",
    "        # Собираем данные\n",
    "        data = []\n",
    "        for record in root.findall('Record'):\n",
    "            date = record.get('Date')\n",
    "            value = record.find('Value').text\n",
    "            nominal = record.find('Nominal').text\n",
    "            \n",
    "            data.append({\n",
    "                'Date': datetime.strptime(date, '%d.%m.%Y'),\n",
    "                'Price': float(value.replace(',', '.')),\n",
    "                'Nominal': int(nominal)\n",
    "            })\n",
    "        \n",
    "        # Создаем DataFrame\n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        # Сортируем по дате\n",
    "        df = df.sort_values('Date')\n",
    "        \n",
    "        # Сохраняем в CSV\n",
    "        df.to_csv('../../../data/external/macro/urals_prices_cbr.csv', index=False)\n",
    "        \n",
    "        print(f\"Данные успешно загружены и сохранены в 'urals_prices_cbr.csv'\")\n",
    "        print(f\"Количество записей: {len(df)}\")\n",
    "        print(\"\\nПервые 5 записей:\")\n",
    "        print(df.head())\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Ошибка при запросе к API ЦБ: {e}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    get_urals_prices_cbr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def download_brent_prices():\n",
    "    # Получаем текущую дату\n",
    "    end_date = datetime.now()\n",
    "    # Дата 10 лет назад\n",
    "    start_date = end_date - timedelta(days=365*10)\n",
    "    \n",
    "    # Загружаем данные для нефти Brent (тикер BZ=F)\n",
    "    brent = yf.download('BZ=F', \n",
    "                        start=start_date.strftime('%Y-%m-%d'),\n",
    "                        end=end_date.strftime('%Y-%m-%d'))\n",
    "    \n",
    "    # Сохраняем в CSV файл\n",
    "    brent.to_csv('extra_features/brent_prices_10y.csv')\n",
    "    \n",
    "    print(f\"Данные сохранены в файл 'brent_prices_10y.csv'\")\n",
    "    print(f\"Количество записей: {len(brent)}\")\n",
    "    \n",
    "    # Показываем первые несколько строк\n",
    "    print(\"\\nПервые 5 записей:\")\n",
    "    print(brent.head())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    download_brent_prices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "# Функция для загрузки данных о курсе доллара за заданный диапазон дат\n",
    "def get_usd_to_rub_exchange_rate(start_date, end_date):\n",
    "    base_url = \"https://www.cbr.ru/scripts/XML_dynamic.asp\"\n",
    "\n",
    "    # Форматирование дат для запроса\n",
    "    start_date_str = start_date.strftime('%d/%m/%Y')\n",
    "    end_date_str = end_date.strftime('%d/%m/%Y')\n",
    "\n",
    "    # Параметры запроса\n",
    "    params = {\n",
    "        'date_req1': start_date_str,\n",
    "        'date_req2': end_date_str,\n",
    "        'VAL_NM_RQ': 'R01235'  # Код валюты для USD\n",
    "    }\n",
    "\n",
    "    # Запрос к API ЦБ РФ\n",
    "    response = requests.get(base_url, params=params)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Ошибка при запросе данных: {response.status_code}\")\n",
    "\n",
    "    # Парсинг ответа XML\n",
    "    data = pd.read_xml(response.content, xpath=\"//Record\")\n",
    "\n",
    "    # Преобразование данных в DataFrame\n",
    "    data['Date'] = pd.to_datetime(data['Date'], format='%d.%m.%Y')\n",
    "    data['Value'] = data['Value'].str.replace(',', '.').astype(float)\n",
    "\n",
    "    # Переименование столбцов для удобства\n",
    "    data = data.rename(columns={\n",
    "        'Date': 'Дата',\n",
    "        'Value': 'Курс USD к RUB'\n",
    "    })\n",
    "\n",
    "    return data\n",
    "\n",
    "# Получение данных за последние 10 лет\n",
    "end_date = datetime.now()\n",
    "start_date = end_date - timedelta(days=10*365)\n",
    "\n",
    "# Загрузка данных\n",
    "try:\n",
    "    exchange_rate_data = get_usd_to_rub_exchange_rate(start_date, end_date)\n",
    "    print(\"Данные успешно загружены!\")\n",
    "    \n",
    "    # Сохранение данных в CSV\n",
    "    output_file = \"../../../data/external/macro/usd_to_rub.csv\"\n",
    "    exchange_rate_data.to_csv(output_file, index=False)\n",
    "    print(\"Данные сохранены в файл 'usd_to_rub.csv'\")\n",
    "except Exception as e:\n",
    "    print(f\"Произошла ошибка: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Загрузка файла с эмбеддингами\n",
    "emb_file = \"../data/external/text/news_with_emb.csv\"\n",
    "df = pd.read_csv(emb_file)\n",
    "\n",
    "# Поиск колонки с эмбеддингами (предполагаем, что это единственная колонка с JSON-строками)\n",
    "emb_cols = df.columns[df.apply(lambda x: x.dtype == 'object' and x.str.contains(r'^\\[.*\\]$').any())]\n",
    "\n",
    "if len(emb_cols) == 1:\n",
    "    # Переименование колонки\n",
    "    old_name = emb_cols[0]\n",
    "    df = df.rename(columns={old_name: 'embedding'})\n",
    "    \n",
    "    # Сохранение обновленного файла\n",
    "    df.to_csv(emb_file, index=False)\n",
    "    print(f\"Колонка '{old_name}' успешно переименована в 'embedding'\")\n",
    "else:\n",
    "    print(\"Не удалось однозначно определить колонку с эмбеддингами\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def get_moex_listed_stocks():\n",
    "    \"\"\"\n",
    "    Fetches a list of actively traded stocks (shares) from the Moscow Exchange API (TQBR board).\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame containing the Ticker (SECID) and Short Name (SHORTNAME)\n",
    "                          of the listed stocks, or None if an error occurs.\n",
    "    \"\"\"\n",
    "    # MOEX ISS API endpoint for securities traded on the TQBR board (main stock board)\n",
    "    url = \"https://iss.moex.com/iss/engines/stock/markets/shares/boards/TQBR/securities.json\"\n",
    "    \n",
    "    params = {\n",
    "        \"iss.meta\": \"off\",          # Turn off metadata block\n",
    "        \"iss.only\": \"securities\",   # Get only the securities data block\n",
    "        \"securities.columns\": \"SECID,SHORTNAME,SECNAME,ISIN,PREVADMITTEDQUOTE\" # Specify desired columns\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, params=params)\n",
    "        response.raise_for_status()  # Raise an exception for bad status codes (4xx or 5xx)\n",
    "        \n",
    "        data = response.json()\n",
    "        \n",
    "        # Extract securities data\n",
    "        securities_data = data['securities']['data']\n",
    "        # Extract column names\n",
    "        columns = data['securities']['columns']\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(securities_data, columns=columns)\n",
    "        \n",
    "        # Filter out securities that are not currently admitted to trading (optional, but good practice)\n",
    "        # PREVADMITTEDQUOTE is often the last traded price, non-null for traded stocks\n",
    "        # A more robust filter might be needed depending on exact MOEX API nuances\n",
    "        df_traded = df[df['PREVADMITTEDQUOTE'].notna()].copy()\n",
    "\n",
    "        # Select relevant columns for the output\n",
    "        df_result = df_traded[['SECID', 'SHORTNAME']].reset_index(drop=True)\n",
    "        \n",
    "        return df_result\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Ошибка запроса к MOEX API: {e}\")\n",
    "        return None\n",
    "    except KeyError as e:\n",
    "        print(f\"Ошибка: Неожиданная структура ответа от MOEX API. Отсутствует ключ: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Произошла непредвиденная ошибка: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Execution ---\n",
    "print(\"Запрос списка акций с Московской Биржи (board TQBR)...\")\n",
    "stocks_df = get_moex_listed_stocks()\n",
    "\n",
    "if stocks_df is not None:\n",
    "    print(f\"\\nНайдено {len(stocks_df)} торгуемых акций на TQBR:\")\n",
    "    # Display the first few and last few rows for brevity\n",
    "    with pd.option_context('display.max_rows', 10):\n",
    "        print(stocks_df)\n",
    "else:\n",
    "    print(\"Не удалось получить список акций.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, requests, collections, time\n",
    "\n",
    "FILE_IN  = '../configs/companies_config.json'\n",
    "FILE_OUT = '../configs/all_companies_config.json'\n",
    "\n",
    "# 1. вытаскиваем все бумаги с борда TQBR\n",
    "url = ('https://iss.moex.com/iss/engines/stock/markets/shares/'\n",
    "       'boards/TQBR/securities.json?iss.meta=off&iss.only=securities')\n",
    "data = requests.get(url, timeout=10).json()['securities']['data']\n",
    "cols = requests.get(url.replace('iss.only=securities','iss.meta=on')\n",
    "                    ).json()['securities']['columns']\n",
    "col = {name: idx for idx, name in enumerate(cols)}\n",
    "\n",
    "full_list = {\n",
    "    row[col['SECID']]: {\n",
    "        \"short\": row[col['SHORTNAME']],\n",
    "        \"full\":  row[col['SECNAME']]\n",
    "    } for row in data\n",
    "}\n",
    "\n",
    "# 2. загружаем ваш файл\n",
    "with open(FILE_IN, encoding='utf‑8') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "have = set(config['companies'])\n",
    "\n",
    "import re\n",
    "\n",
    "# ── вспомогательная функция ──────────────────────────────────────────────────\n",
    "_rm_quotes = re.compile(r'[\\\"«»]')          # удаляем: \" « »\n",
    "def clean(text: str) -> str:\n",
    "    return _rm_quotes.sub('', text).strip()\n",
    "\n",
    "# ── 3. добавляем недостающие бумаги в файл конфигурации ───────────────────────\n",
    "added = 0\n",
    "for secid, names in full_list.items():\n",
    "    if secid in have:\n",
    "        continue\n",
    "\n",
    "    short = clean(names[\"short\"])\n",
    "    full  = clean(names[\"full\"])\n",
    "\n",
    "    config['companies'][secid] = {\n",
    "        \"names\": [\n",
    "            short,\n",
    "            full,\n",
    "            secid      # сам тикер (без изменений)\n",
    "        ]\n",
    "    }\n",
    "    added += 1\n",
    "\n",
    "# 4. сохраняем\n",
    "with open(FILE_OUT, 'w', encoding='utf‑8') as f:\n",
    "    json.dump(config, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f'Добавлено {added} новых компаний → {FILE_OUT}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import json\n",
    "\n",
    "# Выберите нужную модель\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")  \n",
    "\n",
    "total_tokens = 0\n",
    "\n",
    "with open(\"../data/external/text/batch/batch_input_example.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        # Предположим, что текст находится в поле \"text\"\n",
    "        text = data.get(\"text\", \"\")\n",
    "        tokens = encoding.encode(text)\n",
    "        total_tokens += len(tokens)\n",
    "\n",
    "print(f\"Total tokens: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install requests beautifulsoup4 wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, re, requests, wikipedia, bs4, time\n",
    "wikipedia.set_lang(\"ru\")\n",
    "\n",
    "def fetch_aliases(base_name):\n",
    "    try:\n",
    "        page = wikipedia.page(base_name, auto_suggest=False, redirect=True)\n",
    "        soup = bs4.BeautifulSoup(requests.get(page.url).text, \"html.parser\")\n",
    "        first_p = soup.select_one(\"p\").get_text(\" \", strip=True)\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "    # простая эвристика: берём все строки в кавычках/скобках\n",
    "    raw = re.findall(r\"[«\\\"“](.+?)[»\\\"”)]\", first_p) + \\\n",
    "          re.findall(r\"\\(([^)]+)\\)\", first_p)\n",
    "    # подчистим\n",
    "    return {re.sub(r\"\\s+\", \" \", a).strip() for a in raw if 3 < len(a) < 80}\n",
    "\n",
    "with open(\"../configs/all_companies_config.json\", encoding=\"utf-8\") as f:\n",
    "    cfg = json.load(f)\n",
    "\n",
    "for ticker, info in cfg[\"companies\"].items():\n",
    "    base = info[\"names\"][0]\n",
    "    aliases = fetch_aliases(base)\n",
    "    cfg[\"companies\"][ticker][\"names\"] = sorted(set(info[\"names\"]) | aliases)\n",
    "    time.sleep(0.5)          # не злоупотребляем API\n",
    "\n",
    "with open(\"../configs/all_companies_config_enriched.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(cfg, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# Файлы‑источники\n",
    "ALL = Path('../configs/all_companies_config.json')\n",
    "CUR = Path('../configs/companies_config.json')\n",
    "\n",
    "# Регулярка: «ПАО», «АО», «ао» – независимо от регистра, как отдельное слово\n",
    "rm_re = re.compile(r'\\b(?:п?ао)\\b', re.IGNORECASE | re.UNICODE)\n",
    "\n",
    "# Загружаем оба конфигурационных файла\n",
    "with ALL.open(encoding='utf‑8') as f:\n",
    "    all_cfg = json.load(f)\n",
    "\n",
    "with CUR.open(encoding='utf‑8') as f:\n",
    "    cur_cfg = json.load(f)\n",
    "\n",
    "# Тикеры, которых нет во втором файле\n",
    "extra_tickers = set(all_cfg[\"companies\"]) - set(cur_cfg[\"companies\"])\n",
    "\n",
    "for ticker in extra_tickers:\n",
    "    names = all_cfg[\"companies\"][ticker][\"names\"]\n",
    "    new_names = []\n",
    "    for name in names:\n",
    "        # удаляем «ПАО»/«АО»/«ао»\n",
    "        clean = rm_re.sub('', name)\n",
    "        # убираем возможные двойные пробелы и обрезаем края\n",
    "        clean = re.sub(r'\\s{2,}', ' ', clean).strip()\n",
    "        # добавляем пробелы в начале и в конце\n",
    "        new_names.append(f'{clean}')\n",
    "    all_cfg[\"companies\"][ticker][\"names\"] = new_names\n",
    "\n",
    "# Сохраняем результат в новый файл (можно перезаписать исходный, если нужно)\n",
    "out = ALL.with_name('all_companies_config.json')\n",
    "with out.open('w', encoding='utf‑8') as f:\n",
    "    json.dump(all_cfg, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f'Готово! Сохранено в {out}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Пути к файлам\n",
    "config_file_path = '../configs/all_companies_config.json'\n",
    "input_csv_path = '../data/processed/gpt/last_20_rows.csv' # Убедитесь, что это правильный путь к вашему файлу\n",
    "output_csv_path = '../data/processed/gpt/last_20_rows_cleaned.csv'\n",
    "\n",
    "try:\n",
    "    # 1. Загрузка конфигурации\n",
    "    with open(config_file_path, 'r', encoding='utf-8') as f:\n",
    "        config_data = json.load(f)\n",
    "    print(f\"Конфигурационный файл '{config_file_path}' успешно загружен.\")\n",
    "\n",
    "    # 2. Загрузка CSV\n",
    "    try:\n",
    "        df = pd.read_csv(input_csv_path)\n",
    "        print(f\"CSV файл '{input_csv_path}' успешно загружен. Количество строк: {len(df)}, количество колонок: {len(df.columns)}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Ошибка: CSV файл '{input_csv_path}' не найден.\")\n",
    "        exit()\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при чтении CSV файла '{input_csv_path}': {e}\")\n",
    "        exit()\n",
    "\n",
    "    # 3. Создание карты сопоставления (название -> канонический тикер)\n",
    "    name_to_canonical_map = {}\n",
    "    # Компании\n",
    "    for canonical_ticker, company_info in config_data.get('companies', {}).items():\n",
    "        name_to_canonical_map[canonical_ticker.lower()] = canonical_ticker\n",
    "        for name_variant in company_info.get('names', []):\n",
    "            name_to_canonical_map[name_variant.lower()] = canonical_ticker\n",
    "    # Индексы\n",
    "    for canonical_ticker, description in config_data.get('indices', {}).items():\n",
    "        name_to_canonical_map[canonical_ticker.lower()] = canonical_ticker\n",
    "        # name_to_canonical_map[description.lower()] = canonical_ticker # Раскомментируйте, если описания тоже могут быть именами колонок\n",
    "\n",
    "    print(f\"Карта сопоставления создана. Уникальных канонических тикеров в карте: {len(set(name_to_canonical_map.values()))}\")\n",
    "\n",
    "    # Колонки, которые нужно сохранить без изменений\n",
    "    preserved_columns = ['date', 'summary']\n",
    "    cleaned_df_columns = {} # Словарь для сбора колонок нового DataFrame\n",
    "\n",
    "    for col in preserved_columns:\n",
    "        if col in df.columns:\n",
    "            cleaned_df_columns[col] = df[col]\n",
    "        else:\n",
    "            print(f\"Предупреждение: Ожидаемая колонка '{col}' не найдена в CSV.\")\n",
    "\n",
    "    # 4. Идентификация и группировка колонок\n",
    "    column_groups = {}  # {'SBER': ['SBER', 'Sberbank_typo'], ...}\n",
    "    \n",
    "    # Определяем, какие из исходных колонок являются числовыми (кроме preserved_columns)\n",
    "    potential_instrument_cols = [col for col in df.columns if col not in preserved_columns]\n",
    "    \n",
    "    for original_col_name in potential_instrument_cols:\n",
    "        # Попытка приведения к нижнему регистру, если это строка\n",
    "        try:\n",
    "            col_name_lower = str(original_col_name).lower()\n",
    "        except Exception: # На случай если имя колонки не может быть строкой (маловероятно для CSV)\n",
    "            col_name_lower = original_col_name\n",
    "\n",
    "        canonical_ticker = name_to_canonical_map.get(col_name_lower)\n",
    "        \n",
    "        if canonical_ticker:\n",
    "            if canonical_ticker not in column_groups:\n",
    "                column_groups[canonical_ticker] = []\n",
    "            column_groups[canonical_ticker].append(original_col_name)\n",
    "        else:\n",
    "            # Если нужно, можно раскомментировать для отладки ненайденных колонок\n",
    "            print(f\"Информация: Колонка '{original_col_name}' не имеет прямого сопоставления в конфигурации и не будет объединена, если она не числовая или не входит в другую группу.\")\n",
    "\n",
    "    print(f\"Найдено {len(column_groups)} групп колонок для объединения.\")\n",
    "\n",
    "    # 5. Агрегация значений\n",
    "    for canonical_ticker, original_cols_list in column_groups.items():\n",
    "        # Убедимся, что все колонки в группе существуют в DataFrame\n",
    "        existing_cols_in_group = [col for col in original_cols_list if col in df.columns]\n",
    "        \n",
    "        if not existing_cols_in_group:\n",
    "            print(f\"Предупреждение: Для канонического тикера '{canonical_ticker}' не найдено соответствующих колонок в CSV из списка {original_cols_list}.\")\n",
    "            continue\n",
    "\n",
    "        # Преобразуем данные в числовой формат, ошибки заменим на NaN\n",
    "        # Это важно, чтобы .abs() и .idxmax() работали корректно\n",
    "        numeric_data = df[existing_cols_in_group].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "        def get_max_abs_value(row_series):\n",
    "            if row_series.isnull().all():\n",
    "                return np.nan \n",
    "            else:\n",
    "                # .abs() для модуля, .idxmax() для индекса максимального значения в серии модулей\n",
    "                # .loc[] для получения исходного значения (не модуля) по этому индексу\n",
    "                return row_series.loc[row_series.abs().idxmax()]\n",
    "\n",
    "        cleaned_df_columns[canonical_ticker] = numeric_data.apply(get_max_abs_value, axis=1)\n",
    "        \n",
    "    # 6. Формирование итогового DataFrame\n",
    "    final_df = pd.DataFrame(cleaned_df_columns)\n",
    "    \n",
    "    # Упорядочивание колонок: сначала 'date', 'summary', затем остальные по алфавиту\n",
    "    final_cols_order = [col for col in preserved_columns if col in final_df.columns]\n",
    "    instrument_cols = sorted([col for col in final_df.columns if col not in preserved_columns])\n",
    "    final_df = final_df[final_cols_order + instrument_cols]\n",
    "\n",
    "    print(f\"Итоговый DataFrame содержит {len(final_df.columns)} колонок.\")\n",
    "\n",
    "    # 7. Сохранение результата\n",
    "    try:\n",
    "        final_df.to_csv(output_csv_path, index=False, encoding='utf-8')\n",
    "        print(f\"Очищенный DataFrame успешно сохранен в '{output_csv_path}'\")\n",
    "        print(\"\\nПервые 5 строк очищенного файла:\")\n",
    "        print(final_df.head().to_string())\n",
    "        print(f\"\\nПоследние 5 строк очищенного файла:\")\n",
    "        print(final_df.tail().to_string())\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при сохранении итогового CSV файла: {e}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Критическая ошибка: Конфигурационный файл '{config_file_path}' не найден. Скрипт не может продолжить работу.\")\n",
    "except json.JSONDecodeError:\n",
    "    print(f\"Критическая ошибка: Не удалось декодировать JSON из файла '{config_file_path}'. Проверьте его структуру. Скрипт не может продолжить работу.\")\n",
    "except Exception as e:\n",
    "    print(f\"Произошла непредвиденная ошибка во время выполнения скрипта: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Читаем последние 10 строк из CSV файла\n",
    "df = pd.read_csv('../data/processed/gpt/results_gpt_blogs.csv', engine='python').tail(10)\n",
    "\n",
    "# Сохраняем копию в новый файл\n",
    "output_path = '../data/processed/gpt/results_gpt_blogs_last_10.csv'\n",
    "df.to_csv(output_path, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"Копия последних 10 строк сохранена в {output_path}\")\n",
    "print(\"\\nСодержимое файла:\")\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Пути к директориям\n",
    "merged_features_dir = '../data/processed/merged_features'\n",
    "features_final_dir = '../data/features_final'\n",
    "\n",
    "# Получаем списки файлов из обеих директорий\n",
    "merged_files = set([f.split('_merged.csv')[0] for f in os.listdir(merged_features_dir) if f.endswith('_merged.csv')])\n",
    "final_files = set([f.split('_final.csv')[0] for f in os.listdir(features_final_dir) if f.endswith('_final.csv')])\n",
    "\n",
    "# Находим тикеры, которые есть в merged_features, но отсутствуют в features_final\n",
    "missing_tickers = merged_files - final_files\n",
    "\n",
    "print(f\"Тикеры, которые есть в merged_features, но отсутствуют в features_final:\")\n",
    "for ticker in sorted(missing_tickers):\n",
    "    print(f\"- {ticker}\")\n",
    "\n",
    "print(f\"\\nВсего найдено {len(missing_tickers)} отсутствующих тикеров\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
