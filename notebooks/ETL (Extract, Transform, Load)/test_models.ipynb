{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch.nn.functional as F\n",
    "from collections import defaultdict\n",
    "from datetime import timedelta\n",
    "import re\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Предварительная обработка текста без удаления стоп-слов.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^а-яА-Яa-zA-Z0-9\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "# Загрузка модели RuBERT и токенизатора\n",
    "model_name = 'DeepPavlov/rubert-base-cased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Определяем устройство для вычислений\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "def embed_text(texts):\n",
    "    \"\"\"Создает эмбеддинги для списка текстов на GPU.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        encoded_input = tokenizer(texts, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "        model_output = model(**encoded_input)\n",
    "        embeddings = model_output.last_hidden_state[:, 0, :].cpu()  # Перенос на CPU для дальнейшей работы\n",
    "        return embeddings.numpy()\n",
    "\n",
    "\n",
    "def find_duplicates_with_rubert(df, window_days=2, similarity_threshold=0.8):\n",
    "    df = df.copy()\n",
    "    df['datetime'] = pd.to_datetime(df['date'] + ' ' + df['time'])\n",
    "    df = df.sort_values('datetime')\n",
    "    df['processed_text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "    # Создание эмбеддингов для всех текстов\n",
    "    embeddings = embed_text(df['processed_text'].tolist())\n",
    "    \n",
    "    duplicate_groups = defaultdict(set)\n",
    "    processed_indices = set()\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        current_idx = df.index[i]\n",
    "        if current_idx in processed_indices:\n",
    "            continue\n",
    "\n",
    "        current_date = df.iloc[i]['datetime']\n",
    "        date_min = current_date - timedelta(days=window_days)\n",
    "        date_max = current_date + timedelta(days=window_days)\n",
    "\n",
    "        mask_window = (\n",
    "            (df['datetime'] >= date_min) &\n",
    "            (df['datetime'] <= date_max) &\n",
    "            ~df.index.isin(processed_indices)\n",
    "        )\n",
    "        window_indices = df[mask_window].index\n",
    "\n",
    "        if len(window_indices) > 1:\n",
    "            window_embeddings = embeddings[window_indices]\n",
    "\n",
    "            emb1 = torch.tensor([embeddings[i]])\n",
    "            emb2 = torch.tensor(window_embeddings)\n",
    "\n",
    "            similarities = F.cosine_similarity(emb1, emb2).numpy()\n",
    "\n",
    "            similar_indices = np.where(similarities > similarity_threshold)[0]\n",
    "            if len(similar_indices) > 1:\n",
    "                similar_indices_full = window_indices[similar_indices]\n",
    "                earliest_idx = df.loc[similar_indices_full, 'datetime'].idxmin()\n",
    "                duplicate_groups[earliest_idx].update(\n",
    "                    idx for idx in similar_indices_full if idx != earliest_idx\n",
    "                )\n",
    "                processed_indices.update(similar_indices_full)\n",
    "\n",
    "    all_duplicates = set(idx for d in duplicate_groups.values() for idx in d)\n",
    "    df_cleaned = df.drop(index=list(all_duplicates))\n",
    "    df_cleaned.drop(['datetime', 'processed_text'], axis=1, inplace=True)\n",
    "\n",
    "    logging.info(f\"Найдено {len(all_duplicates)} дубликатов в {len(duplicate_groups)} группах\")\n",
    "    logging.info(f\"Осталось {len(df_cleaned)} уникальных новостей\")\n",
    "\n",
    "    return df_cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Читаем CSV файл\n",
    "    df = pd.read_csv('chunk_news.csv')\n",
    "    \n",
    "    # Удаляем дубликаты с временным окном в 2 дня\n",
    "    df_cleaned = find_duplicates_with_rubert(df, window_days=1, similarity_threshold=0.8)\n",
    "    \n",
    "    # Сохраняем результат\n",
    "    df_cleaned.to_csv('test_news.csv', index=False)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import defaultdict\n",
    "from datetime import timedelta\n",
    "import re\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Загрузка модели RuBERT и токенизатора\n",
    "model_name = 'DeepPavlov/rubert-base-cased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Определяем устройство для вычислений\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Предварительная обработка текста без удаления стоп-слов.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^а-яА-Яa-zA-Z0-9\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def embed_text(texts):\n",
    "    \"\"\"Создает эмбеддинги для списка текстов на GPU.\"\"\"\n",
    "    filtered_texts = []\n",
    "    skipped_count = 0\n",
    "\n",
    "    # Пропускаем тексты длиннее 512 токенов\n",
    "    for text in texts:\n",
    "        tokenized_length = len(tokenizer.tokenize(text))\n",
    "        if tokenized_length <= 512:\n",
    "            filtered_texts.append(text)\n",
    "        else:\n",
    "            skipped_count += 1\n",
    "\n",
    "    logging.info(f\"Пропущено {skipped_count} текстов из-за превышения 512 токенов\")\n",
    "\n",
    "    if not filtered_texts:\n",
    "        return None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoded_input = tokenizer(\n",
    "            filtered_texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        ).to(device)\n",
    "        model_output = model(**encoded_input)\n",
    "        embeddings = model_output.last_hidden_state[:, 0, :].cpu()\n",
    "        return embeddings.numpy()\n",
    "\n",
    "\n",
    "def find_duplicates_with_rubert(df, window_days=2, similarity_threshold=0.8):\n",
    "    df = df.copy()\n",
    "    df['datetime'] = pd.to_datetime(df['date'] + ' ' + df['time'])\n",
    "    df = df.sort_values('datetime')\n",
    "    df['processed_text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "    # Создание эмбеддингов для всех текстов\n",
    "    embeddings = embed_text(df['processed_text'].tolist())\n",
    "\n",
    "    if embeddings is None:\n",
    "        logging.warning(\"Все тексты были пропущены. Нет данных для обработки.\")\n",
    "        return df\n",
    "\n",
    "    duplicate_groups = defaultdict(set)\n",
    "    processed_indices = set()\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        current_idx = df.index[i]\n",
    "        if current_idx in processed_indices:\n",
    "            continue\n",
    "\n",
    "        current_date = df.iloc[i]['datetime']\n",
    "        date_min = current_date - timedelta(days=window_days)\n",
    "        date_max = current_date + timedelta(days=window_days)\n",
    "\n",
    "        mask_window = (\n",
    "            (df['datetime'] >= date_min) &\n",
    "            (df['datetime'] <= date_max) &\n",
    "            ~df.index.isin(processed_indices)\n",
    "        )\n",
    "        window_indices = df[mask_window].index\n",
    "\n",
    "        if len(window_indices) > 1:\n",
    "            window_embeddings = embeddings[window_indices]\n",
    "            similarities = cosine_similarity([embeddings[i]], window_embeddings)[0]\n",
    "\n",
    "            similar_indices = np.where(similarities > similarity_threshold)[0]\n",
    "            if len(similar_indices) > 1:\n",
    "                similar_indices_full = window_indices[similar_indices]\n",
    "                earliest_idx = df.loc[similar_indices_full, 'datetime'].idxmin()\n",
    "                duplicate_groups[earliest_idx].update(\n",
    "                    idx for idx in similar_indices_full if idx != earliest_idx\n",
    "                )\n",
    "                processed_indices.update(similar_indices_full)\n",
    "\n",
    "    all_duplicates = set(idx for d in duplicate_groups.values() for idx in d)\n",
    "    df_cleaned = df.drop(index=list(all_duplicates))\n",
    "    df_cleaned.drop(['datetime', 'processed_text'], axis=1, inplace=True)\n",
    "\n",
    "    logging.info(f\"Найдено {len(all_duplicates)} дубликатов в {len(duplicate_groups)} группах\")\n",
    "    logging.info(f\"Осталось {len(df_cleaned)} уникальных новостей\")\n",
    "\n",
    "    return df_cleaned\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Читаем CSV файл\n",
    "    df = pd.read_csv('chunk_news.csv')\n",
    "    \n",
    "    # Удаляем дубликаты с временным окном в 1 день\n",
    "    df_cleaned = find_duplicates_with_rubert(df, window_days=1, similarity_threshold=0.8)\n",
    "    \n",
    "    # Сохраняем результат\n",
    "    df_cleaned.to_csv('test_news.csv', index=False)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Читаем CSV файл\n",
    "    df = pd.read_csv('chunk_news.csv')\n",
    "    \n",
    "    # Удаляем дубликаты с временным окном в 1 день\n",
    "    df_cleaned = find_duplicates_with_rubert(df, window_days=1, similarity_threshold=0.8)\n",
    "    \n",
    "    # Сохраняем результат\n",
    "    df_cleaned.to_csv('test_news.csv', index=False)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (PyTorch GPU)",
   "language": "python",
   "name": "pytorch_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
